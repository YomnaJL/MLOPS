import pytest
import pandas as pd
from unittest.mock import patch, MagicMock
import sys
import os

# Configuration des chemins
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.abspath(os.path.join(current_dir, '../backend/src')))

from api import load_best_model

@patch('api.mlflow')
@patch('api.pickle')
@patch('builtins.open', new_callable=MagicMock)
def test_load_best_model_logic(mock_open, mock_pickle, mock_mlflow):
    print("\n\n" + "="*60)
    print("üé¨ D√âBUT DU SC√âNARIO DE TEST")
    print("="*60)

    # --- 1. MISE EN SC√àNE (MOCKS) ---
    print("ü§ñ 1. Simulation : Cr√©ation de 3 faux mod√®les...")
    
    # On simule que l'exp√©rience existe
    mock_experiment = MagicMock()
    mock_experiment.experiment_id = "EXP_001"
    mock_mlflow.get_experiment_by_name.return_value = mock_experiment

    # On cr√©e notre faux tableau de r√©sultats (Le Run B est le meilleur)
    fake_runs = pd.DataFrame({
        'run_id': ['RUN_B_WINNER', 'RUN_C_AVG', 'RUN_A_BAD'],
        'metrics.f1_weighted': [0.92, 0.85, 0.50],
        'tags.model_name': ['XGBoost', 'RandomForest', 'LogisticRegression'],
        'tags.stage': ['Tuned', 'Baseline', 'Baseline']
    })
    
    # On dit √† MLflow (le faux) : "Quand on te demande les runs, donne cette liste"
    mock_mlflow.search_runs.return_value = fake_runs
    print(f"üìä Donn√©es simul√©es envoy√©es √† l'API :\n{fake_runs[['run_id', 'metrics.f1_weighted', 'tags.model_name']]}")

    # On simule le contenu du fichier mod√®le
    fake_model_content = "Je suis l'objet mod√®le XGBoost"
    mock_pickle.load.return_value = fake_model_content

    # --- 2. ACTION ---
    print("\nüèÉ 2. Action : L'API appelle la fonction 'load_best_model()'...")
    model, name = load_best_model()

    # --- 3. V√âRIFICATION ---
    print("\nüïµÔ∏è 3. V√©rification : Qui a √©t√© choisi ?")
    
    # V√©rification du tri
    args, kwargs = mock_mlflow.search_runs.call_args
    print(f"   üëâ Crit√®re de tri utilis√© par l'API : {kwargs['order_by']}")

    # V√©rification du t√©l√©chargement
    # On r√©cup√®re les arguments avec lesquels download_artifacts a √©t√© appel√©
    call_args = mock_mlflow.artifacts.download_artifacts.call_args
    downloaded_run_id = call_args.kwargs['run_id']
    downloaded_file = call_args.kwargs['artifact_path']

    print(f"   üëâ L'API a t√©l√©charg√© le Run ID : '{downloaded_run_id}'")
    print(f"   üëâ L'API a cherch√© le fichier   : '{downloaded_file}'")

    # TEST FINAL
    if downloaded_run_id == 'RUN_B_WINNER':
        print("\n‚úÖ SUCC√àS : L'API a bien pris le mod√®le avec le meilleur score (0.92) !")
    else:
        print(f"\n‚ùå √âCHEC : L'API a pris {downloaded_run_id} au lieu de RUN_B_WINNER.")
        pytest.fail("Mauvais mod√®le s√©lectionn√©")

    print("="*60 + "\n")